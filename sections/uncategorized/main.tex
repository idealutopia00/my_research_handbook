\chapter{Visual Recognition Evolution}

\section{Convolutional Neural Networks}
For a long time, ResNet \cite{He2016ResNet} served as the backbone for most vision tasks due to its ability to solve the vanishing gradient problem in deep networks.

\section{Vision Transformers (ViT)}
Inspired by NLP success, the Vision Transformer \cite{Dosovitskiy2020ViT} splits images into patches, treating them as sequences, challenging the dominance of CNNs.

\chapter{Foundations of LLMs}

\section{The Transformer Architecture}
The field of Natural Language Processing was revolutionized by the introduction of the Transformer architecture \cite{Vaswani2017Attention}. 
This mechanism allows for parallelization tailored for modern hardware.

\section{Scaling Laws and GPT}
Recent developments have shown that scaling up model size and data volume leads to emergent abilities, as demonstrated in the GPT-4 technical report \cite{OpenAI2023GPT4}.

\chapter{Algorithmic Trading Strategies}

\section{Market Efficiency}
The Efficient Market Hypothesis (EMH) \cite{Fama1970Market} suggests that asset prices reflect all available information.

\section{Machine Learning in Finance}
Modern quantitative finance is moving towards ML-based approaches. As noted by Lopez de Prado \cite{Lopez2018Finance}, applying standard ML cross-validation in finance leads to overfitting due to time-series correlation.